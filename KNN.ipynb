{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4819a4b7-0fb6-45ef-989f-370f790c69de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Memory-Based Collaborative Filtering model trained.\n",
      "ðŸ“Š Collaborative Filtering Evaluation Metrics:\n",
      "Mean Absolute Error (MAE): 0.0381\n",
      "Root Mean Square Error (RMSE): 0.1542\n",
      "\n",
      "âœ… Collaborative Filtering model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# --------------------------\n",
    "# 1. LOAD THE DATASET\n",
    "# --------------------------\n",
    "df = pd.read_csv('./data/categorized_specializations.csv')\n",
    "\n",
    "# Handle skew by removing low-count university names\n",
    "min_samples_per_class = 50\n",
    "valid_classes = df['univName'].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= min_samples_per_class].index\n",
    "df = df[df['univName'].isin(valid_classes)]\n",
    "# One-hot encoding univ_state \n",
    "df = pd.get_dummies(df, columns=['univ_state'], drop_first=True)\n",
    "\n",
    "# Label encoding categorical columns\n",
    "def encode_categorical_columns(df, categorical_columns):\n",
    "    label_encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = df[col].astype(str)\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le\n",
    "    return df, label_encoders\n",
    "\n",
    "# Scaling continuous columns\n",
    "def scale_numerical_columns(df, numerical_columns):\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_columns] = df[numerical_columns].replace([np.inf, -np.inf], np.nan)\n",
    "    df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].mean())\n",
    "    df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "    return df, scaler\n",
    "\n",
    "# --------------------------\n",
    "# 2. MEMORY-BASED COLLABORATIVE FILTERING CLASS\n",
    "# --------------------------\n",
    "class MemoryBasedCF:\n",
    "    \"\"\"\n",
    "    Memory-based Collaborative Filtering using cosine similarity.\n",
    "    The model is built on a pivot table of users and items (universities) with normalized ratings.\n",
    "    \"\"\"\n",
    "    def __init__(self, pivot):\n",
    "        self.pivot = pivot\n",
    "        # Compute cosine similarity between all users in the pivot table.\n",
    "        self.similarity = cosine_similarity(pivot)\n",
    "        self.user_ids = list(pivot.index)\n",
    "        self.item_ids = list(pivot.columns)\n",
    "\n",
    "    def predict(self, input_user):\n",
    "        \"\"\"\n",
    "        Given a user name, computes a weighted average of ratings from similar users.\n",
    "        Returns a probability distribution over items.\n",
    "        \"\"\"\n",
    "        if input_user in self.user_ids:\n",
    "            idx = self.user_ids.index(input_user)\n",
    "            sim_scores = self.similarity[idx]\n",
    "        else:\n",
    "            # If user not found, assume equal similarity.\n",
    "            sim_scores = np.ones(len(self.user_ids)) / len(self.user_ids)\n",
    "        \n",
    "        # Weighted sum of ratings from similar users.\n",
    "        weighted_sum = np.dot(sim_scores, self.pivot.values)\n",
    "        # Normalize the predictions\n",
    "        if weighted_sum.sum() > 0:\n",
    "            preds = weighted_sum / weighted_sum.sum()\n",
    "        else:\n",
    "            preds = weighted_sum\n",
    "        return preds\n",
    "\n",
    "# --------------------------\n",
    "# 3. TRAINING THE CF MODEL\n",
    "# --------------------------\n",
    "def train_collaborative_filtering_memory(df, user_col, item_col, rating_col):\n",
    "    \"\"\"\n",
    "    Trains a memory-based collaborative filtering model:\n",
    "      - Creates a pivot table for users and items.\n",
    "      - Normalizes ratings to be between 0 and 1.\n",
    "      - Computes cosine similarity.\n",
    "    Returns an instance of MemoryBasedCF.\n",
    "    \"\"\"\n",
    "    df[user_col] = df[user_col].astype(str)\n",
    "    df[item_col] = df[item_col].astype(str)\n",
    "    \n",
    "    # Normalize the rating column to a [0, 1] scale.\n",
    "    df[rating_col] = (df[rating_col] - df[rating_col].min()) / (df[rating_col].max() - df[rating_col].min())\n",
    "    \n",
    "    df_pivot = df.pivot_table(index=user_col, columns=item_col, values=rating_col, aggfunc='mean', fill_value=0)\n",
    "    cf_model = MemoryBasedCF(df_pivot)\n",
    "    print(\"âœ… Memory-Based Collaborative Filtering model trained.\")\n",
    "    return cf_model, df_pivot\n",
    "\n",
    "# --------------------------\n",
    "# 4. EVALUATION METRICS\n",
    "# --------------------------\n",
    "def evaluate_cf_model(cf_model, df_pivot):\n",
    "    \"\"\"\n",
    "    Evaluates the collaborative filtering model using Mean Absolute Error (MAE) and \n",
    "    Root Mean Square Error (RMSE) based on the predicted and actual ratings.\n",
    "    \"\"\"\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "    \n",
    "    for user in df_pivot.index:\n",
    "        actual = df_pivot.loc[user].values\n",
    "        predicted = cf_model.predict(user)\n",
    "        \n",
    "        # Store values for error calculation\n",
    "        actual_ratings.extend(actual)\n",
    "        predicted_ratings.extend(predicted)\n",
    "    \n",
    "    # Compute MAE and RMSE\n",
    "    mae = mean_absolute_error(actual_ratings, predicted_ratings)\n",
    "    rmse = np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))\n",
    "\n",
    "    print(f\"ðŸ“Š Collaborative Filtering Evaluation Metrics:\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"Root Mean Square Error (RMSE): {rmse:.4f}\")\n",
    "    \n",
    "    return mae, rmse\n",
    "\n",
    "# --------------------------\n",
    "# 5. RUN TRAINING & EVALUATION\n",
    "# --------------------------\n",
    "cf_model, df_pivot = train_collaborative_filtering_memory(df, 'userName', 'univName', 'admit')\n",
    "\n",
    "# Evaluate the CF model\n",
    "mae, rmse = evaluate_cf_model(cf_model, df_pivot)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(cf_model, \"models/cf_model_memory.pkl\")\n",
    "\n",
    "print(\"\\nâœ… Collaborative Filtering model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f01f86-fd06-4a4d-886f-04ec977b589f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All models and preprocessing objects loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# --------------------------\n",
    "# 1. LOAD SAVED MODELS & PREPROCESSING OBJECTS\n",
    "# --------------------------\n",
    "save_dir = \"models/university_models\"\n",
    "\n",
    "# Load the trained Random Forest model (saved as a dict; we extract the model from the tuple)\n",
    "models_univ = joblib.load(os.path.join(save_dir, 'rf_university.pkl'))\n",
    "rf_model = models_univ['Random Forest'][0]  # content-based model\n",
    "\n",
    "# Load collaborative filtering model (memory-based CF)\n",
    "cf_model = joblib.load(os.path.join(save_dir, 'cf_model_memory.pkl'))\n",
    "\n",
    "# Load preprocessing objects\n",
    "scaler_univ = joblib.load(os.path.join(save_dir, 'scaler_university.pkl'))\n",
    "label_encoders_univ = joblib.load(os.path.join(save_dir, 'label_encoders_university.pkl'))\n",
    "le_y_univ = joblib.load(os.path.join(save_dir, 'le_y_univ.pkl'))\n",
    "one_hot_columns = joblib.load(os.path.join(save_dir, \"one_hot_columns_university.pkl\"))\n",
    "\n",
    "print(\"âœ… All models and preprocessing objects loaded successfully.\")\n",
    "\n",
    "# Define the feature set used during training:\n",
    "univ_categorical = ['ugCollege', 'specialization_category']\n",
    "univ_numerical   = ['toeflScore', 'greV', 'greQ', 'greA', 'normalized_cgpa']\n",
    "\n",
    "# --------------------------\n",
    "# 2. HYBRID RECOMMENDER FUNCTION\n",
    "# --------------------------\n",
    "def hybrid_university_recommendation(input_df, content_model, collab_model, label_encoder, one_hot_columns):\n",
    "    \"\"\"\n",
    "    Generates university recommendations by combining:\n",
    "      - Content-based predictions (using the Random Forest classifier).\n",
    "      - Collaborative filtering predictions (using memory-based CF).\n",
    "    Returns the top 3 recommended universities (decoded).\n",
    "    \"\"\"\n",
    "    # Remove \"userName\" column if present (content model was trained without it)\n",
    "    input_features = input_df.drop(columns=['userName'], errors='ignore')\n",
    "    \n",
    "    # --- Content-based predictions ---\n",
    "    try:\n",
    "        content_probs = content_model.predict_proba(input_features)[0]\n",
    "    except Exception:\n",
    "        content_probs = content_model.predict(input_features)\n",
    "    \n",
    "    # --- Collaborative filtering predictions ---\n",
    "    # For CF, we use the userName if present; otherwise, use zeros.\n",
    "    if 'userName' in input_df.columns:\n",
    "        input_user = input_df.iloc[0]['userName']\n",
    "        collab_probs = collab_model.predict(input_user)\n",
    "    else:\n",
    "        collab_probs = np.zeros_like(content_probs)\n",
    "    \n",
    "    # --- Combine predictions using variance-based weighting ---\n",
    "    alpha = np.var(content_probs) / (np.var(content_probs) + np.var(collab_probs) + 1e-5)\n",
    "    final_probs = (content_probs * alpha) + (collab_probs * (1 - alpha))\n",
    "    \n",
    "    # Select top 3 recommendations\n",
    "    top_n = 3\n",
    "    top_indices = np.argsort(final_probs)[-top_n:][::-1]\n",
    "    recommendations = label_encoder.inverse_transform(top_indices)\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# --------------------------\n",
    "# 3. EVALUATION FUNCTION FOR THE HYBRID MODEL\n",
    "# --------------------------\n",
    "def evaluate_hybrid_model(X_test, y_test, rf_model, cf_model, le_y, one_hot_columns):\n",
    "    \"\"\"\n",
    "    Evaluates the hybrid recommender system by:\n",
    "      - For each test sample, obtaining hybrid recommendations.\n",
    "      - Taking the top recommendation as the predicted label.\n",
    "      - Comparing against the true label.\n",
    "    Prints accuracy and a classification report.\n",
    "    \"\"\"\n",
    "    y_test_encoded = le_y.transform(y_test)\n",
    "    y_pred = []\n",
    "    \n",
    "    for i in range(len(X_test)):\n",
    "        sample = X_test.iloc[[i]].copy()\n",
    "        # Add a dummy \"userName\" column (required for CF) if not present.\n",
    "        if 'userName' not in sample.columns:\n",
    "            sample['userName'] = \"dummy_user\"\n",
    "        \n",
    "        recs = hybrid_university_recommendation(sample, rf_model, cf_model, le_y, one_hot_columns)\n",
    "        # Use the top recommendation as the prediction\n",
    "        if len(recs) > 0:\n",
    "            pred = le_y.transform([recs[0]])[0]\n",
    "        else:\n",
    "            pred = -1\n",
    "        y_pred.append(pred)\n",
    "    \n",
    "    # Remove invalid predictions if any\n",
    "    valid_idx = [i for i, p in enumerate(y_pred) if p != -1]\n",
    "    y_true_valid = np.array([y_test_encoded[i] for i in valid_idx])\n",
    "    y_pred_valid = np.array([y_pred[i] for i in valid_idx])\n",
    "    \n",
    "    acc = accuracy_score(y_true_valid, y_pred_valid)\n",
    "    print(\"\\nðŸ“Š Hybrid Model Evaluation Results:\")\n",
    "    print(f\"Hybrid Model Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_true_valid, y_pred_valid))\n",
    "\n",
    "# --------------------------\n",
    "# 4. PREPROCESS TEST DATA\n",
    "# --------------------------\n",
    "df_test = pd.read_csv('./data/categorized_specializations.csv')\n",
    "\n",
    "# Filter out universities with too few samples (as in training)\n",
    "min_samples_per_class = 50\n",
    "valid_classes = df_test['univName'].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= min_samples_per_class].index\n",
    "df_test = df_test[df_test['univName'].isin(valid_classes)]\n",
    "\n",
    "# --- Apply label encoding for categorical columns ---\n",
    "def encode_categorical_columns(df, categorical_columns):\n",
    "    label_encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = df[col].astype(str)\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le\n",
    "    return df, label_encoders\n",
    "\n",
    "def scale_numerical_columns(df, numerical_columns):\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_columns] = df[numerical_columns].replace([np.inf, -np.inf], np.nan)\n",
    "    df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].mean())\n",
    "    df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "    return df, scaler\n",
    "\n",
    "df_test, _ = encode_categorical_columns(df_test, univ_categorical)\n",
    "df_test, _ = scale_numerical_columns(df_test, univ_numerical)\n",
    "# One-hot encode the \"univ_state\" column.\n",
    "df_test = pd.get_dummies(df_test, columns=['univ_state'], drop_first=True)\n",
    "# Ensure all one-hot columns from training exist in test data.\n",
    "for col in one_hot_columns:\n",
    "    if col not in df_test.columns:\n",
    "        df_test[col] = 0\n",
    "# Reorder columns to match training features.\n",
    "X_test = df_test[univ_categorical + univ_numerical + one_hot_columns]\n",
    "y_test = df_test['univName']\n",
    "\n",
    "# --------------------------\n",
    "# 5. EVALUATE THE HYBRID RECOMMENDER\n",
    "# --------------------------\n",
    "evaluate_hybrid_model(X_test, y_test, rf_model, cf_model, le_y_univ, one_hot_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bdc7e-e7a6-4bae-8af7-297d0d99c554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
