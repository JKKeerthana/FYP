{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4819a4b7-0fb6-45ef-989f-370f790c69de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Memory-Based Collaborative Filtering model trained.\n",
      "📊 Collaborative Filtering Evaluation Metrics:\n",
      "Mean Absolute Error (MAE): 0.0381\n",
      "Root Mean Square Error (RMSE): 0.1542\n",
      "\n",
      "✅ Collaborative Filtering model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# --------------------------\n",
    "# 1. LOAD THE DATASET\n",
    "# --------------------------\n",
    "df = pd.read_csv('./data/categorized_specializations.csv')\n",
    "\n",
    "# Handle skew by removing low-count university names\n",
    "min_samples_per_class = 50\n",
    "valid_classes = df['univName'].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= min_samples_per_class].index\n",
    "df = df[df['univName'].isin(valid_classes)]\n",
    "# One-hot encoding univ_state \n",
    "df = pd.get_dummies(df, columns=['univ_state'], drop_first=True)\n",
    "\n",
    "# Label encoding categorical columns\n",
    "def encode_categorical_columns(df, categorical_columns):\n",
    "    label_encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = df[col].astype(str)\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le\n",
    "    return df, label_encoders\n",
    "\n",
    "# Scaling continuous columns\n",
    "def scale_numerical_columns(df, numerical_columns):\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_columns] = df[numerical_columns].replace([np.inf, -np.inf], np.nan)\n",
    "    df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].mean())\n",
    "    df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "    return df, scaler\n",
    "\n",
    "# --------------------------\n",
    "# 2. MEMORY-BASED COLLABORATIVE FILTERING CLASS\n",
    "# --------------------------\n",
    "class MemoryBasedCF:\n",
    "    \"\"\"\n",
    "    Memory-based Collaborative Filtering using cosine similarity.\n",
    "    The model is built on a pivot table of users and items (universities) with normalized ratings.\n",
    "    \"\"\"\n",
    "    def __init__(self, pivot):\n",
    "        self.pivot = pivot\n",
    "        # Compute cosine similarity between all users in the pivot table.\n",
    "        self.similarity = cosine_similarity(pivot)\n",
    "        self.user_ids = list(pivot.index)\n",
    "        self.item_ids = list(pivot.columns)\n",
    "\n",
    "    def predict(self, input_user):\n",
    "        \"\"\"\n",
    "        Given a user name, computes a weighted average of ratings from similar users.\n",
    "        Returns a probability distribution over items.\n",
    "        \"\"\"\n",
    "        if input_user in self.user_ids:\n",
    "            idx = self.user_ids.index(input_user)\n",
    "            sim_scores = self.similarity[idx]\n",
    "        else:\n",
    "            # If user not found, assume equal similarity.\n",
    "            sim_scores = np.ones(len(self.user_ids)) / len(self.user_ids)\n",
    "        \n",
    "        # Weighted sum of ratings from similar users.\n",
    "        weighted_sum = np.dot(sim_scores, self.pivot.values)\n",
    "        # Normalize the predictions\n",
    "        if weighted_sum.sum() > 0:\n",
    "            preds = weighted_sum / weighted_sum.sum()\n",
    "        else:\n",
    "            preds = weighted_sum\n",
    "        return preds\n",
    "\n",
    "# --------------------------\n",
    "# 3. TRAINING THE CF MODEL\n",
    "# --------------------------\n",
    "def train_collaborative_filtering_memory(df, user_col, item_col, rating_col):\n",
    "    \"\"\"\n",
    "    Trains a memory-based collaborative filtering model:\n",
    "      - Creates a pivot table for users and items.\n",
    "      - Normalizes ratings to be between 0 and 1.\n",
    "      - Computes cosine similarity.\n",
    "    Returns an instance of MemoryBasedCF.\n",
    "    \"\"\"\n",
    "    df[user_col] = df[user_col].astype(str)\n",
    "    df[item_col] = df[item_col].astype(str)\n",
    "    \n",
    "    # Normalize the rating column to a [0, 1] scale.\n",
    "    df[rating_col] = (df[rating_col] - df[rating_col].min()) / (df[rating_col].max() - df[rating_col].min())\n",
    "    \n",
    "    df_pivot = df.pivot_table(index=user_col, columns=item_col, values=rating_col, aggfunc='mean', fill_value=0)\n",
    "    cf_model = MemoryBasedCF(df_pivot)\n",
    "    print(\"✅ Memory-Based Collaborative Filtering model trained.\")\n",
    "    return cf_model, df_pivot\n",
    "\n",
    "# --------------------------\n",
    "# 4. EVALUATION METRICS\n",
    "# --------------------------\n",
    "def evaluate_cf_model(cf_model, df_pivot):\n",
    "    \"\"\"\n",
    "    Evaluates the collaborative filtering model using Mean Absolute Error (MAE) and \n",
    "    Root Mean Square Error (RMSE) based on the predicted and actual ratings.\n",
    "    \"\"\"\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "    \n",
    "    for user in df_pivot.index:\n",
    "        actual = df_pivot.loc[user].values\n",
    "        predicted = cf_model.predict(user)\n",
    "        \n",
    "        # Store values for error calculation\n",
    "        actual_ratings.extend(actual)\n",
    "        predicted_ratings.extend(predicted)\n",
    "    \n",
    "    # Compute MAE and RMSE\n",
    "    mae = mean_absolute_error(actual_ratings, predicted_ratings)\n",
    "    rmse = np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))\n",
    "\n",
    "    print(f\"📊 Collaborative Filtering Evaluation Metrics:\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"Root Mean Square Error (RMSE): {rmse:.4f}\")\n",
    "    \n",
    "    return mae, rmse\n",
    "\n",
    "# --------------------------\n",
    "# 5. RUN TRAINING & EVALUATION\n",
    "# --------------------------\n",
    "cf_model, df_pivot = train_collaborative_filtering_memory(df, 'userName', 'univName', 'admit')\n",
    "\n",
    "# Evaluate the CF model\n",
    "mae, rmse = evaluate_cf_model(cf_model, df_pivot)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(cf_model, \"models/cf_model_memory.pkl\")\n",
    "\n",
    "print(\"\\n✅ Collaborative Filtering model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7f01f86-fd06-4a4d-886f-04ec977b589f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All models and preprocessing objects loaded successfully.\n",
      "\n",
      "📊 Hybrid Model Evaluation Results:\n",
      "Hybrid Model Accuracy: 0.8558\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      3727\n",
      "           1       0.54      0.69      0.60        54\n",
      "           2       0.87      0.86      0.87      1600\n",
      "           3       1.00      1.00      1.00       933\n",
      "           4       0.57      0.67      0.62       518\n",
      "           5       0.65      0.69      0.67       685\n",
      "           6       0.89      0.93      0.91       461\n",
      "           7       1.00      1.00      1.00      2368\n",
      "           8       0.85      0.84      0.85       228\n",
      "           9       0.79      0.82      0.80       100\n",
      "          10       0.93      0.96      0.94       545\n",
      "          11       0.59      0.62      0.61       502\n",
      "          12       0.94      0.95      0.94      4149\n",
      "          13       0.93      0.93      0.93      2242\n",
      "          14       0.79      0.79      0.79       112\n",
      "          15       0.91      0.92      0.92      1114\n",
      "          16       0.98      0.83      0.90        59\n",
      "          17       1.00      1.00      1.00      1270\n",
      "          18       0.95      0.93      0.94       619\n",
      "          19       0.53      0.67      0.59       422\n",
      "          20       0.76      0.71      0.73      2636\n",
      "          21       0.69      0.68      0.68      1852\n",
      "          22       0.69      0.71      0.70      1445\n",
      "          23       0.78      0.73      0.75      2455\n",
      "          24       0.89      0.80      0.84      1041\n",
      "          25       0.64      0.92      0.75       284\n",
      "          26       0.67      0.59      0.63      1187\n",
      "          27       0.51      0.55      0.53       652\n",
      "          28       0.56      0.55      0.55       809\n",
      "          29       0.54      0.56      0.55       608\n",
      "          30       0.54      0.73      0.62       176\n",
      "          31       0.93      0.92      0.92      1236\n",
      "          32       1.00      1.00      1.00       912\n",
      "          33       1.00      1.00      1.00      2743\n",
      "          34       0.95      0.94      0.94      2064\n",
      "          35       0.87      0.88      0.88       940\n",
      "          36       0.97      0.97      0.97      1245\n",
      "          37       0.87      0.86      0.87      1037\n",
      "          38       0.99      0.98      0.98       793\n",
      "          39       1.00      1.00      1.00      1540\n",
      "          40       0.77      0.68      0.72       116\n",
      "          41       0.83      0.81      0.82      1237\n",
      "          42       0.79      0.80      0.79      1009\n",
      "          43       0.78      0.71      0.74      2253\n",
      "          44       0.72      0.69      0.71      1251\n",
      "          45       0.70      0.76      0.73      1293\n",
      "          46       0.81      0.82      0.81      3761\n",
      "          47       1.00      1.00      1.00       724\n",
      "          48       1.00      1.00      1.00       514\n",
      "          49       1.00      1.00      1.00      1097\n",
      "          50       0.97      0.96      0.97      1191\n",
      "          51       0.92      0.94      0.93       189\n",
      "          52       0.73      0.81      0.77       159\n",
      "\n",
      "    accuracy                           0.86     62157\n",
      "   macro avg       0.82      0.83      0.83     62157\n",
      "weighted avg       0.86      0.86      0.86     62157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# --------------------------\n",
    "# 1. LOAD SAVED MODELS & PREPROCESSING OBJECTS\n",
    "# --------------------------\n",
    "save_dir = \"models/university_models\"\n",
    "\n",
    "# Load the trained Random Forest model (saved as a dict; we extract the model from the tuple)\n",
    "models_univ = joblib.load(os.path.join(save_dir, 'rf_university.pkl'))\n",
    "rf_model = models_univ['Random Forest'][0]  # content-based model\n",
    "\n",
    "# Load collaborative filtering model (memory-based CF)\n",
    "cf_model = joblib.load(os.path.join(save_dir, 'cf_model_memory.pkl'))\n",
    "\n",
    "# Load preprocessing objects\n",
    "scaler_univ = joblib.load(os.path.join(save_dir, 'scaler_university.pkl'))\n",
    "label_encoders_univ = joblib.load(os.path.join(save_dir, 'label_encoders_university.pkl'))\n",
    "le_y_univ = joblib.load(os.path.join(save_dir, 'le_y_univ.pkl'))\n",
    "one_hot_columns = joblib.load(os.path.join(save_dir, \"one_hot_columns_university.pkl\"))\n",
    "\n",
    "print(\"✅ All models and preprocessing objects loaded successfully.\")\n",
    "\n",
    "# Define the feature set used during training:\n",
    "univ_categorical = ['ugCollege', 'specialization_category']\n",
    "univ_numerical   = ['toeflScore', 'greV', 'greQ', 'greA', 'normalized_cgpa']\n",
    "\n",
    "# --------------------------\n",
    "# 2. HYBRID RECOMMENDER FUNCTION\n",
    "# --------------------------\n",
    "def hybrid_university_recommendation(input_df, content_model, collab_model, label_encoder, one_hot_columns):\n",
    "    \"\"\"\n",
    "    Generates university recommendations by combining:\n",
    "      - Content-based predictions (using the Random Forest classifier).\n",
    "      - Collaborative filtering predictions (using memory-based CF).\n",
    "    Returns the top 3 recommended universities (decoded).\n",
    "    \"\"\"\n",
    "    # Remove \"userName\" column if present (content model was trained without it)\n",
    "    input_features = input_df.drop(columns=['userName'], errors='ignore')\n",
    "    \n",
    "    # --- Content-based predictions ---\n",
    "    try:\n",
    "        content_probs = content_model.predict_proba(input_features)[0]\n",
    "    except Exception:\n",
    "        content_probs = content_model.predict(input_features)\n",
    "    \n",
    "    # --- Collaborative filtering predictions ---\n",
    "    # For CF, we use the userName if present; otherwise, use zeros.\n",
    "    if 'userName' in input_df.columns:\n",
    "        input_user = input_df.iloc[0]['userName']\n",
    "        collab_probs = collab_model.predict(input_user)\n",
    "    else:\n",
    "        collab_probs = np.zeros_like(content_probs)\n",
    "    \n",
    "    # --- Combine predictions using variance-based weighting ---\n",
    "    alpha = np.var(content_probs) / (np.var(content_probs) + np.var(collab_probs) + 1e-5)\n",
    "    final_probs = (content_probs * alpha) + (collab_probs * (1 - alpha))\n",
    "    \n",
    "    # Select top 3 recommendations\n",
    "    top_n = 3\n",
    "    top_indices = np.argsort(final_probs)[-top_n:][::-1]\n",
    "    recommendations = label_encoder.inverse_transform(top_indices)\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# --------------------------\n",
    "# 3. EVALUATION FUNCTION FOR THE HYBRID MODEL\n",
    "# --------------------------\n",
    "def evaluate_hybrid_model(X_test, y_test, rf_model, cf_model, le_y, one_hot_columns):\n",
    "    \"\"\"\n",
    "    Evaluates the hybrid recommender system by:\n",
    "      - For each test sample, obtaining hybrid recommendations.\n",
    "      - Taking the top recommendation as the predicted label.\n",
    "      - Comparing against the true label.\n",
    "    Prints accuracy and a classification report.\n",
    "    \"\"\"\n",
    "    y_test_encoded = le_y.transform(y_test)\n",
    "    y_pred = []\n",
    "    \n",
    "    for i in range(len(X_test)):\n",
    "        sample = X_test.iloc[[i]].copy()\n",
    "        # Add a dummy \"userName\" column (required for CF) if not present.\n",
    "        if 'userName' not in sample.columns:\n",
    "            sample['userName'] = \"dummy_user\"\n",
    "        \n",
    "        recs = hybrid_university_recommendation(sample, rf_model, cf_model, le_y, one_hot_columns)\n",
    "        # Use the top recommendation as the prediction\n",
    "        if len(recs) > 0:\n",
    "            pred = le_y.transform([recs[0]])[0]\n",
    "        else:\n",
    "            pred = -1\n",
    "        y_pred.append(pred)\n",
    "    \n",
    "    # Remove invalid predictions if any\n",
    "    valid_idx = [i for i, p in enumerate(y_pred) if p != -1]\n",
    "    y_true_valid = np.array([y_test_encoded[i] for i in valid_idx])\n",
    "    y_pred_valid = np.array([y_pred[i] for i in valid_idx])\n",
    "    \n",
    "    acc = accuracy_score(y_true_valid, y_pred_valid)\n",
    "    print(\"\\n📊 Hybrid Model Evaluation Results:\")\n",
    "    print(f\"Hybrid Model Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_true_valid, y_pred_valid))\n",
    "\n",
    "# --------------------------\n",
    "# 4. PREPROCESS TEST DATA\n",
    "# --------------------------\n",
    "df_test = pd.read_csv('./data/categorized_specializations.csv')\n",
    "\n",
    "# Filter out universities with too few samples (as in training)\n",
    "min_samples_per_class = 50\n",
    "valid_classes = df_test['univName'].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= min_samples_per_class].index\n",
    "df_test = df_test[df_test['univName'].isin(valid_classes)]\n",
    "\n",
    "# --- Apply label encoding for categorical columns ---\n",
    "def encode_categorical_columns(df, categorical_columns):\n",
    "    label_encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = df[col].astype(str)\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le\n",
    "    return df, label_encoders\n",
    "\n",
    "def scale_numerical_columns(df, numerical_columns):\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_columns] = df[numerical_columns].replace([np.inf, -np.inf], np.nan)\n",
    "    df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].mean())\n",
    "    df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "    return df, scaler\n",
    "\n",
    "df_test, _ = encode_categorical_columns(df_test, univ_categorical)\n",
    "df_test, _ = scale_numerical_columns(df_test, univ_numerical)\n",
    "# One-hot encode the \"univ_state\" column.\n",
    "df_test = pd.get_dummies(df_test, columns=['univ_state'], drop_first=True)\n",
    "# Ensure all one-hot columns from training exist in test data.\n",
    "for col in one_hot_columns:\n",
    "    if col not in df_test.columns:\n",
    "        df_test[col] = 0\n",
    "# Reorder columns to match training features.\n",
    "X_test = df_test[univ_categorical + univ_numerical + one_hot_columns]\n",
    "y_test = df_test['univName']\n",
    "\n",
    "# --------------------------\n",
    "# 5. EVALUATE THE HYBRID RECOMMENDER\n",
    "# --------------------------\n",
    "evaluate_hybrid_model(X_test, y_test, rf_model, cf_model, le_y_univ, one_hot_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab9fb515-5585-4254-8128-e918f8c02d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hybrid_model(X_test, y_test, rf_model, cf_model, le_y, one_hot_columns):\n",
    "    \"\"\"\n",
    "    Evaluates the hybrid recommender system by:\n",
    "      - For each test sample, obtaining hybrid recommendations.\n",
    "      - Taking the top recommendation as the predicted label.\n",
    "      - Comparing against the true label.\n",
    "    Prints accuracy, classification report, and predictions vs actual values.\n",
    "    \"\"\"\n",
    "    y_test_encoded = le_y.transform(y_test)\n",
    "    y_pred = []\n",
    "    predictions_vs_actuals = []\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        sample = X_test.iloc[[i]].copy()\n",
    "        # Add a dummy \"userName\" column (required for CF) if not present.\n",
    "        if 'userName' not in sample.columns:\n",
    "            sample['userName'] = \"dummy_user\"\n",
    "        \n",
    "        recs = hybrid_university_recommendation(sample, rf_model, cf_model, le_y, one_hot_columns)\n",
    "        # Use the top recommendation as the prediction\n",
    "        if len(recs) > 0:\n",
    "            pred = le_y.transform([recs[0]])[0]\n",
    "            pred_univ = recs[0]\n",
    "        else:\n",
    "            pred = -1\n",
    "            pred_univ = \"None\"\n",
    "        y_pred.append(pred)\n",
    "        \n",
    "        actual_univ = y_test.iloc[i]\n",
    "        predictions_vs_actuals.append((actual_univ, pred_univ))\n",
    "\n",
    "    # Remove invalid predictions if any\n",
    "    valid_idx = [i for i, p in enumerate(y_pred) if p != -1]\n",
    "    y_true_valid = np.array([y_test_encoded[i] for i in valid_idx])\n",
    "    y_pred_valid = np.array([y_pred[i] for i in valid_idx])\n",
    "    \n",
    "    acc = accuracy_score(y_true_valid, y_pred_valid)\n",
    "    print(\"\\n📊 Hybrid Model Evaluation Results:\")\n",
    "    print(f\"Hybrid Model Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_true_valid, y_pred_valid))\n",
    "\n",
    "    # Print predictions vs actual values\n",
    "    print(\"\\n🔍 Predictions vs Actual Values:\")\n",
    "    print(\"Actual University  |  Predicted University\")\n",
    "    print(\"-\" * 50)\n",
    "    for actual, predicted in predictions_vs_actuals[:20]:  # Print only first 20 for readability\n",
    "        print(f\"{actual:25} | {predicted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bdc7e-e7a6-4bae-8af7-297d0d99c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE  \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "############################################\n",
    "# Collaborative Filtering Functions\n",
    "############################################\n",
    "\n",
    "def compute_similarity(data):\n",
    "    \"\"\"\n",
    "    Compute user-user similarity using cosine similarity.\n",
    "    :param data: Pandas DataFrame where rows are users and columns are university names.\n",
    "    :return: User-user similarity matrix.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(cosine_similarity(data), index=data.index, columns=data.index)\n",
    "\n",
    "def predict_major(user_id, data, similarity_matrix, top_n=1):\n",
    "    \"\"\"\n",
    "    Predict the most likely university for a user based on similar users.\n",
    "    :param user_id: Index of the user in the DataFrame.\n",
    "    :param data: Original user-university preference DataFrame.\n",
    "    :param similarity_matrix: Precomputed user-user similarity matrix.\n",
    "    :param top_n: Number of top universities to recommend.\n",
    "    :return: List of recommended universities.\n",
    "    \"\"\"\n",
    "    # Find similar users (excluding the user itself)\n",
    "    similar_users = similarity_matrix[user_id].drop(user_id).nlargest(5)\n",
    "    \n",
    "    # Aggregate university selections among similar users (weighted sum)\n",
    "    weighted_scores = data.loc[similar_users.index].T.dot(similar_users)\n",
    "    \n",
    "    # Recommend top universities based on weighted scores\n",
    "    recommended_univs = weighted_scores.nlargest(top_n).index.tolist()\n",
    "    \n",
    "    return recommended_univs\n",
    "\n",
    "def build_collaborative_filtering_model(df, user_col, univ_col):\n",
    "    \"\"\"\n",
    "    Build a collaborative filtering model using a pivot table.\n",
    "    We count the number of times a user selected a university.\n",
    "    :param df: DataFrame with user and university info.\n",
    "    :param user_col: Column name for user identifier.\n",
    "    :param univ_col: Column name for university name.\n",
    "    :return: similarity_matrix, user_univ_matrix\n",
    "    \"\"\"\n",
    "    # Build pivot table: rows = users, columns = universities; values = counts (or binary indicator)\n",
    "    user_univ_matrix = pd.crosstab(df[user_col], df[univ_col])\n",
    "    similarity_matrix = compute_similarity(user_univ_matrix)\n",
    "    return similarity_matrix, user_univ_matrix\n",
    "\n",
    "############################################\n",
    "# Content-Based Preprocessing and Model Training\n",
    "############################################\n",
    "\n",
    "def encode_categorical_columns(df, categorical_columns):\n",
    "    label_encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = df[col].astype(str)\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le\n",
    "    return df, label_encoders\n",
    "\n",
    "def scale_numerical_columns(df, numerical_columns):\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_columns] = df[numerical_columns].replace([np.inf, -np.inf], np.nan)\n",
    "    df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].mean())\n",
    "    df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "    return df, scaler\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "# Hybrid Recommendation Function\n",
    "############################################\n",
    "\n",
    "def hybrid_university_recommendation(user_id, input_df, content_model, similarity_matrix, user_univ_matrix, label_encoder,\n",
    "                                     is_university=False, state=None, one_hot_columns=None):\n",
    "    if is_university:\n",
    "        # --- Content-Based Prediction ---\n",
    "        try:\n",
    "            if state is None and one_hot_columns is not None:\n",
    "                probs_list = []\n",
    "                for state_col in one_hot_columns:\n",
    "                    temp_df = input_df.copy()\n",
    "                    temp_df[one_hot_columns] = 0  # reset all state one-hot columns\n",
    "                    temp_df[state_col] = 1         # set current state\n",
    "                    try:\n",
    "                        probs = content_model.predict_proba(temp_df)[0]\n",
    "                    except Exception as e:\n",
    "                        probs = content_model.predict(temp_df)\n",
    "                    probs_list.append(probs)\n",
    "                content_probs = np.mean(probs_list, axis=0)\n",
    "            else:\n",
    "                try:\n",
    "                    content_probs = content_model.predict_proba(input_df)[0]\n",
    "                except Exception as e:\n",
    "                    content_probs = content_model.predict(input_df)\n",
    "        except Exception as e:\n",
    "            print(\"Error in content model prediction:\", e)\n",
    "            content_probs = np.zeros(len(label_encoder.classes_))\n",
    "        \n",
    "        # --- Collaborative Filtering Prediction ---\n",
    "        try:\n",
    "            collab_recs = predict_major(user_id, user_univ_matrix, similarity_matrix, top_n=3)\n",
    "            collab_probs = np.zeros_like(content_probs)\n",
    "            for rec in collab_recs:\n",
    "                rec_index = np.where(label_encoder.classes_ == rec)[0]\n",
    "                if len(rec_index) > 0:\n",
    "                    collab_probs[rec_index] += 1\n",
    "            collab_probs = collab_probs / (len(collab_recs) if len(collab_recs) > 0 else 1)\n",
    "        except Exception as e:\n",
    "            print(\"Error in collaborative filtering:\", e)\n",
    "            collab_probs = np.zeros_like(content_probs)\n",
    "        \n",
    "        # --- Combine the Two Predictions ---\n",
    "        alpha = np.var(content_probs) / (np.var(content_probs) + np.var(collab_probs) + 1e-5)\n",
    "        final_probs = (content_probs * alpha) + (collab_probs * (1 - alpha))\n",
    "        \n",
    "        top_n = 3\n",
    "        top_indices = np.argsort(final_probs)[-top_n:][::-1]\n",
    "        final_recommendations = label_encoder.inverse_transform(top_indices)\n",
    "        \n",
    "        if state is not None:\n",
    "            # If filtering by state is needed, adjust this filtering step as per your one-hot column naming\n",
    "            filtered_recommendations = [rec for rec in final_recommendations \n",
    "                                        if rec in df[df['univ_state'] == state]['univName'].values]\n",
    "            if len(filtered_recommendations) < top_n:\n",
    "                print(f\"Only {len(filtered_recommendations)} universities found in {state}. Expanding recommendations.\")\n",
    "                filtered_recommendations = final_recommendations\n",
    "            return filtered_recommendations\n",
    "        \n",
    "        return final_recommendations\n",
    "\n",
    "############################################\n",
    "# Main Section\n",
    "############################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Load Dataset ---\n",
    "    df = pd.read_csv('./data/categorized_specializations.csv')\n",
    "    \n",
    "    # Filter to keep only university names with at least 50 samples\n",
    "    min_samples_per_class = 50\n",
    "    valid_classes = df['univName'].value_counts()\n",
    "    valid_classes = valid_classes[valid_classes >= min_samples_per_class].index\n",
    "    df = df[df['univName'].isin(valid_classes)]\n",
    "    \n",
    "    # One-hot encode the \"univ_state\" column\n",
    "    df = pd.get_dummies(df, columns=['univ_state'], drop_first=True)\n",
    "    one_hot_columns = [col for col in df.columns if col.startswith(\"univ_state_\")]\n",
    "    \n",
    "    # --- Prepare Data for Content-Based Model ---\n",
    "    univ_categorical = ['ugCollege', 'specialization_category']\n",
    "    univ_numerical = ['toeflScore', 'greV', 'greQ', 'greA', 'normalized_cgpa']\n",
    "    \n",
    "    df, label_encoders_univ = encode_categorical_columns(df, univ_categorical)\n",
    "    df, scaler_univ = scale_numerical_columns(df, univ_numerical)\n",
    "    \n",
    "    # Build the feature set (include one-hot state columns)\n",
    "    X_univ = df[univ_categorical + univ_numerical + one_hot_columns]\n",
    "    y_univ = df['univName']\n",
    "    \n",
    "    X_univ_train, X_univ_test, y_univ_train, y_univ_test = train_test_split(\n",
    "        X_univ, y_univ, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Apply SMOTE on the training set\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_univ_train_resampled, y_univ_train_resampled = smote.fit_resample(X_univ_train, y_univ_train)\n",
    "    \n",
    "    # --- Load or Train Content-Based Model ---\n",
    "    save_dir = \"models/university_models\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        models_univ = joblib.load(os.path.join(save_dir, 'rf_university.pkl'))\n",
    "        scaler_univ = joblib.load(os.path.join(save_dir, 'scaler_university.pkl'))\n",
    "        label_encoders_univ = joblib.load(os.path.join(save_dir, 'label_encoders_university.pkl'))\n",
    "        le_y_univ = joblib.load(os.path.join(save_dir, 'le_y_univ.pkl'))\n",
    "        print(\"Loaded saved content-based models.\")\n",
    "    except Exception as e:\n",
    "        print(\"Saved models not found. Training content-based model.\")\n",
    "        models_univ, le_y_univ = train_models(X_univ_train_resampled, X_univ_test, y_univ_train_resampled, y_univ_test, 'University')\n",
    "        joblib.dump(models_univ, os.path.join(save_dir, 'rf_university.pkl'))\n",
    "        joblib.dump(scaler_univ, os.path.join(save_dir, 'scaler_university.pkl'))\n",
    "        joblib.dump(label_encoders_univ, os.path.join(save_dir, 'label_encoders_university.pkl'))\n",
    "        joblib.dump(le_y_univ, os.path.join(save_dir, 'le_y_univ.pkl'))\n",
    "    \n",
    "    content_model, _ = models_univ['Random Forest']\n",
    "    \n",
    "    # --- Build Collaborative Filtering Model ---\n",
    "    # Here we build a pivot table based on user selections of \"univName\"\n",
    "    similarity_matrix, user_univ_matrix = build_collaborative_filtering_model(df, 'userName', 'univName')\n",
    "    \n",
    "    # --- Example Hybrid Recommendation ---\n",
    "    # Pick a sample user from the collaborative filtering pivot table\n",
    "    sample_user = user_univ_matrix.index[0]\n",
    "    # For content-based input, we select one row from the test set\n",
    "    sample_input = X_univ_test.iloc[[0]]  # keep as a DataFrame\n",
    "    \n",
    "    recommendations = hybrid_university_recommendation(\n",
    "        user_id=sample_user,\n",
    "        input_df=sample_input,\n",
    "        content_model=content_model,\n",
    "        similarity_matrix=similarity_matrix,\n",
    "        user_univ_matrix=user_univ_matrix,\n",
    "        label_encoder=le_y_univ,\n",
    "        is_university=True,\n",
    "        state=None,\n",
    "        one_hot_columns=one_hot_columns\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nRecommended Universities for user '{sample_user}':\", recommendations)\n",
    "    \n",
    "    # Optionally, save one_hot_columns if needed\n",
    "    joblib.dump(one_hot_columns, os.path.join(save_dir, \"one_hot_columns_university.pkl\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
